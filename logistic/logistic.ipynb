{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f850bc",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "## Brief:\n",
    "In this notebook I will be implementing a logistic regression model using just numpy, that classifies a cancer patient's tumor as malignant or benign using multiple variable (Multivariate Logistic Regression).  \n",
    "#### Goals:\n",
    "- [Extracting Data](#data-extraction)\n",
    "- [Defining Logistic Model](#logistic-regression-model)\n",
    "- [Normalizing Data](#normalization-of-data)\n",
    "- [Cost Function](#cost-function)\n",
    "- [Gradient Descent](#gradient-descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5246bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96154871",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "The dataset i'll be using is a very interesting one called **\"Diagnostic Wisconsin Breast Cancer Database.\"**  \n",
    "Which contains: <center>*\"Features that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\"*</center><center>*\"They describe characteristics of the cell nuclei present in the image.\"*</center>\n",
    "\n",
    "In breif, it's a datset that contains 33 features with the target value classifying a patient's tumor as \"Malignent\" or \"Benign\".  \n",
    "Since the target array ${y}$ currently contains \"M\" for malignent and \"B\" for benign, we'll turn the array into boolean form with M = 1 and B = 0.  \n",
    "\n",
    "Citation:\n",
    "> ðŸ“Š **Dataset Reference**  \n",
    "> Wolberg, W., Mangasarian, O., Street, N., & Street, W. (1993). *Breast Cancer Wisconsin (Diagnostic)* [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af4d8bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
       "0      17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
       "1      20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
       "2      19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
       "3      11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
       "4      20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
       "..       ...       ...         ...     ...          ...           ...   \n",
       "564    21.56     22.39      142.00  1479.0      0.11100       0.11590   \n",
       "565    20.13     28.25      131.20  1261.0      0.09780       0.10340   \n",
       "566    16.60     28.08      108.30   858.1      0.08455       0.10230   \n",
       "567    20.60     29.33      140.10  1265.0      0.11780       0.27700   \n",
       "568     7.76     24.54       47.92   181.0      0.05263       0.04362   \n",
       "\n",
       "     concavity1  concave_points1  symmetry1  fractal_dimension1  ...  radius3  \\\n",
       "0       0.30010          0.14710     0.2419             0.07871  ...   25.380   \n",
       "1       0.08690          0.07017     0.1812             0.05667  ...   24.990   \n",
       "2       0.19740          0.12790     0.2069             0.05999  ...   23.570   \n",
       "3       0.24140          0.10520     0.2597             0.09744  ...   14.910   \n",
       "4       0.19800          0.10430     0.1809             0.05883  ...   22.540   \n",
       "..          ...              ...        ...                 ...  ...      ...   \n",
       "564     0.24390          0.13890     0.1726             0.05623  ...   25.450   \n",
       "565     0.14400          0.09791     0.1752             0.05533  ...   23.690   \n",
       "566     0.09251          0.05302     0.1590             0.05648  ...   18.980   \n",
       "567     0.35140          0.15200     0.2397             0.07016  ...   25.740   \n",
       "568     0.00000          0.00000     0.1587             0.05884  ...    9.456   \n",
       "\n",
       "     texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0       17.33      184.60  2019.0      0.16220       0.66560      0.7119   \n",
       "1       23.41      158.80  1956.0      0.12380       0.18660      0.2416   \n",
       "2       25.53      152.50  1709.0      0.14440       0.42450      0.4504   \n",
       "3       26.50       98.87   567.7      0.20980       0.86630      0.6869   \n",
       "4       16.67      152.20  1575.0      0.13740       0.20500      0.4000   \n",
       "..        ...         ...     ...          ...           ...         ...   \n",
       "564     26.40      166.10  2027.0      0.14100       0.21130      0.4107   \n",
       "565     38.25      155.00  1731.0      0.11660       0.19220      0.3215   \n",
       "566     34.12      126.70  1124.0      0.11390       0.30940      0.3403   \n",
       "567     39.42      184.60  1821.0      0.16500       0.86810      0.9387   \n",
       "568     30.37       59.16   268.6      0.08996       0.06444      0.0000   \n",
       "\n",
       "     concave_points3  symmetry3  fractal_dimension3  \n",
       "0             0.2654     0.4601             0.11890  \n",
       "1             0.1860     0.2750             0.08902  \n",
       "2             0.2430     0.3613             0.08758  \n",
       "3             0.2575     0.6638             0.17300  \n",
       "4             0.1625     0.2364             0.07678  \n",
       "..               ...        ...                 ...  \n",
       "564           0.2216     0.2060             0.07115  \n",
       "565           0.1628     0.2572             0.06637  \n",
       "566           0.1418     0.2218             0.07820  \n",
       "567           0.2650     0.4087             0.12400  \n",
       "568           0.0000     0.2871             0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract data using pd.read_csv\n",
    "\n",
    "x_raw, y_raw = pd.read_csv(\"wdbc.data\", sep=',').drop(['ID', 'Diagnosis'], axis=1), pd.read_csv(\"wdbc.data\", usecols=[1])\n",
    "x_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738645e",
   "metadata": {},
   "source": [
    "As you can see, we successfully extracted the data into a ``x_raw`` variable. (Which contains elements of varying sizes, so normalization is needed. More on that in [Normalization of Data](#normalization-of-data))  \n",
    "However, the ``y_raw`` variable contains letters as seen in this sample:  \n",
    "| Index | Diagnosis |\n",
    "|-------|-----------|\n",
    "| 0     | M         |\n",
    "| 1     | M         |\n",
    "| 2     | M         |\n",
    "| 3     | M         |\n",
    "| 4     | M         |\n",
    "| ...   | ...       |\n",
    "| 564   | M         |\n",
    "| 565   | M         |\n",
    "| 566   | M         |\n",
    "| 567   | M         |\n",
    "| 568   | B         |\n",
    "\n",
    "Therefore, we will convert the elements in the Diagnosis column into boolean values using ``numpy.where(condition, true then x, else y)``  Where <center>\"M\" = 1 || \"B\" = 0</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc63f1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Diagnosis\n",
       "0            1\n",
       "1            1\n",
       "2            1\n",
       "3            1\n",
       "4            1\n",
       "..         ...\n",
       "564          1\n",
       "565          1\n",
       "566          1\n",
       "567          1\n",
       "568          0\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.where(y_raw.values == \"M\", 1, 0).reshape(-1)\n",
    "pd.DataFrame(y, columns=['Diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416152c",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "The logistic regression model, also known as the Binary Classification Model (since it outputs either 1 or 0 after applying a threshold), itself is a quite simple one, it takes *input* the linear model we're very used to, and plugs it's negative as the exponential's power.  \n",
    "The logistic regression model is defined as:\n",
    "\n",
    "$$\n",
    "sigmoid(z) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- ${z}$ is the linear model ${w \\cdot x + b}$\n",
    "- ${sigmoid()}$ is another name for the logistic model\n",
    "\n",
    "And this results in a graph like such:  \n",
    "\n",
    "<center><img src=\"1.webp\" height=\"300\" width=\"300\"></center>\n",
    "\n",
    "The logistic model calculates the **Probability**, it's hypothesis is: \"What is the propability of it being 1 or 0\". An example output of a logistic model could be something like 0.78, which would mean a 78% probability of the input x being classified as 1 or \"True\".\n",
    "This is wonderful for **Binary Clasification**, as by simply setting a threshold (say, 0.5) the output of this model could is limited to 0 or 1. (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dae93ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af13af",
   "metadata": {},
   "source": [
    "## Normalization of Data\n",
    "\n",
    "In this dataset, we have **30 features!!** Because of this, and especially since this is a logistic regression model, normalizing the data is a must.  \n",
    "For this case, we will use *Z-Score Normalization*  \n",
    "As before normalizing our features, their variation in size was large. But after normalization, they all vary from -1 to 1.  \n",
    "- Before normalization: ``x[1] = [20.57   17.77   132.9   1326   0.08]`` (Some are small ``0.08`` and some are large ``1326``)\n",
    "- After normalization: ``x[1] = [-0.181 -0.193  0.311  5.537 -0.271]`` (Same data, ranges from -1 to 1)\n",
    "\n",
    "Z-score is defined as:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original data point\n",
    "- $\\mu$ is the mean\n",
    "- $\\sigma$ is the deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5874296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x):\n",
    "    return (x - np.mean(x)) / np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fefe5",
   "metadata": {},
   "source": [
    "## Logarithmic Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e17da7",
   "metadata": {},
   "source": [
    "The Logarthmic Cost Function, also known as **Binary Cross-Entropy** because of its use of boolean digits. This cost function is special to specific regression models such as Logistic Regression (This current model) and the RELU activation model (Rectified Linear Unit) since their outputs, after a threshold is put in place, is boolean.\n",
    "\n",
    "The error for ${f(x)}$ if ${y = 1}$ is ${-\\log(f(x))}$ Which results in the graph that shows the variation of cost according to ${f(x)}$ in document **(c)**. As you can see, as the output of the model deviates from the truth label ${y = 1}$ and approaches 0, the error outputted increased to the highest point being ${-\\log(0)}$ (which is a math error so a very small ${\\epsilon}$ is added)  \n",
    "The logarithmic cost function strongly penalizes confident but incorrect predictions. For example, if the model predicts a very high probability for the wrong class, the cost will be high\n",
    "\n",
    "The error for ${f(x)}$ if ${y = 0}$ is ${-\\log(1 - f(x))}$ Which is shown in document **(b)**. Which also increases the cost the further you mvoe from the ground truth ${y}$\n",
    "\n",
    "\n",
    "<center><img src=\"https://i.sstatic.net/ufmSH.png\"></center>\n",
    "\n",
    "So, in code, the Logarthmic Cost (Binary Cross-Entropy) is defined as:\n",
    "\n",
    "$$\n",
    "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- ${\\hat{y}}$ is the model's prediction (1 or 0)\n",
    "- ${y}$ is the ground truth label (1 or 0)\n",
    "- ${m}$ is the number of training examples\n",
    "\n",
    "Notes:\n",
    "- A very small ${\\epsilon}$ of ``1e-7`` is added to prevent the case of ${\\log(0)}$, (which is a math error, very bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7392b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, b, x, y):\n",
    "    f_wb = sigmoid(x @ w + b) # Sigmoid function (calculates probability)\n",
    "    epsilon = 1e-7\n",
    "    f_wb = np.clip(f_wb, epsilon, 1 - epsilon) # Limits output of f_wb to prevent -log(0)\n",
    "    err = y * np.log(f_wb + epsilon) + (1 - y) * np.log(1 - f_wb + epsilon)\n",
    "    return -np.mean(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73018c",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent for logistic regression does not differ much from linear regression, the only thing that differs is the way of calculating ${f(x)}$ (now that we are using the Sigmoid Function [Sigmoid Function](#logistic-regression-model))\n",
    "\n",
    "So the gradient descent algorithm is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial j(w,b)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial j(w,b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where in linear regression, ${\\frac{\\partial j(w,b)}{\\partial w_j}}$ is calculated using ${x \\cdot w + b}$  \n",
    "But now in logistic regression, that ${x \\cdot w + b}$ is plugging into ${\\frac{1}{1 + e^{-(w \\cdot x + b)}}}$\n",
    "\n",
    "Notes:\n",
    "- ${j(w,b)}$ is the model error\n",
    "- ${\\alpha}$ is the learning rate alpha (usually a very small number ~ 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c7601e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(w, b, x, y, alphar, itterations):\n",
    "    err_arr = []\n",
    "    for j in range(itterations):\n",
    "        dj_dw = 0.\n",
    "        dj_db = 0.\n",
    "        for i in range(x.shape[0]):\n",
    "            f_wb = sigmoid(x[i,:] @ w + b)\n",
    "            f_wb = np.clip(f_wb, 1e-7, 1 - 1e-7)\n",
    "            dj_dw += (f_wb - y[i]) * x[i]\n",
    "            dj_db += (f_wb - y[i])\n",
    "        w = w - alphar * (dj_dw / x.shape[0])\n",
    "        b = b - alphar * (dj_db / x.shape[0])\n",
    "        ## Ignore, for visualization purposes ##\n",
    "        current_cost = cost(w, b, x, y)\n",
    "        err_arr.append(current_cost)\n",
    "        if j % 100 == 0: print(f\"epoch {j} || Current Cost = {current_cost}\")\n",
    "        ## Ignore, for visualization purposes ##\n",
    "\n",
    "    return w, b, err_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4a384267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 || Current Cost = 0.3227144065732773\n",
      "epoch 100 || Current Cost = 0.21374656331653663\n",
      "epoch 200 || Current Cost = 0.1958659103755996\n",
      "epoch 300 || Current Cost = 0.18726150136888844\n",
      "epoch 400 || Current Cost = 0.18186513274462685\n",
      "epoch 500 || Current Cost = 0.17805496036071097\n",
      "epoch 600 || Current Cost = 0.17518554276640969\n",
      "epoch 700 || Current Cost = 0.17293691884092185\n",
      "epoch 800 || Current Cost = 0.17112632397926217\n",
      "epoch 900 || Current Cost = 0.16963870974081088\n",
      "epoch 1000 || Current Cost = 0.1683965511195969\n",
      "epoch 1100 || Current Cost = 0.16734502341237234\n",
      "epoch 1200 || Current Cost = 0.16644398399116736\n",
      "epoch 1300 || Current Cost = 0.16566327896653713\n",
      "epoch 1400 || Current Cost = 0.16497981423683572\n",
      "epoch 1500 || Current Cost = 0.16437563166041105\n",
      "epoch 1600 || Current Cost = 0.163836593600256\n",
      "epoch 1700 || Current Cost = 0.16335145456477007\n",
      "epoch 1800 || Current Cost = 0.16291118974295887\n",
      "epoch 1900 || Current Cost = 0.16250849980401627\n",
      "epoch 2000 || Current Cost = 0.16213743984785534\n",
      "epoch 2100 || Current Cost = 0.1617931375587907\n",
      "epoch 2200 || Current Cost = 0.16147157692184178\n",
      "epoch 2300 || Current Cost = 0.16116943041241819\n",
      "epoch 2400 || Current Cost = 0.1608839277413544\n",
      "epoch 2500 || Current Cost = 0.16061275237239875\n",
      "epoch 2600 || Current Cost = 0.1603539592994509\n",
      "epoch 2700 || Current Cost = 0.16010590930504706\n",
      "epoch 2800 || Current Cost = 0.1598672160590909\n",
      "epoch 2900 || Current Cost = 0.15963670330122343\n",
      "epoch 3000 || Current Cost = 0.15941336999558145\n",
      "epoch 3100 || Current Cost = 0.15919636182868224\n",
      "epoch 3200 || Current Cost = 0.1589849477842041\n",
      "epoch 3300 || Current Cost = 0.1587785008040125\n",
      "epoch 3400 || Current Cost = 0.15857648175932007\n",
      "epoch 3500 || Current Cost = 0.15837842610503755\n",
      "epoch 3600 || Current Cost = 0.1581839327123376\n",
      "epoch 3700 || Current Cost = 0.15799265455476744\n",
      "epoch 3800 || Current Cost = 0.15780429084508557\n",
      "epoch 3900 || Current Cost = 0.15761858037418025\n",
      "epoch 4000 || Current Cost = 0.15743529593680342\n",
      "epoch 4100 || Current Cost = 0.1572542395608496\n",
      "epoch 4200 || Current Cost = 0.1570752384607732\n",
      "epoch 4300 || Current Cost = 0.1568981415879405\n",
      "epoch 4400 || Current Cost = 0.15672281668327356\n",
      "epoch 4500 || Current Cost = 0.15654914775374615\n",
      "epoch 4600 || Current Cost = 0.15637703290750588\n",
      "epoch 4700 || Current Cost = 0.15620638249320043\n",
      "epoch 4800 || Current Cost = 0.15603711749796983\n",
      "epoch 4900 || Current Cost = 0.1558691681658761\n"
     ]
    }
   ],
   "source": [
    "w = np.random.rand(30)\n",
    "b = np.random.rand(1)\n",
    "x = z_score(x_raw.values)\n",
    "w, b, err_arr = calculate_gradient(w, b, x, y, 0.5, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4d569f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(5000), err_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57dc9dd",
   "metadata": {},
   "source": [
    "## Final Chapter: Testing the trained model\n",
    "Now that we've run gradient descent and made the binary classification model coverge, we'll put it up to the test!! \n",
    "\n",
    "We'll do two fun tests:\n",
    "- Test the model on a patient to see if it classifies them correctly\n",
    "- Measure how often does a model get it's classifications wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bca294",
   "metadata": {},
   "source": [
    "For the first patient, we'll log the information of their tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d97a536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.1189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
       "0    17.99     10.38       122.8  1001.0       0.1184        0.2776   \n",
       "\n",
       "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  radius3  \\\n",
       "0      0.3001           0.1471     0.2419             0.07871  ...    25.38   \n",
       "\n",
       "   texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0     17.33       184.6  2019.0       0.1622        0.6656      0.7119   \n",
       "\n",
       "   concave_points3  symmetry3  fractal_dimension3  \n",
       "0           0.2654     0.4601              0.1189  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_line = \"842302,M,17.99,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189\"\n",
    "columns = [\n",
    "    \"radius1\", \"texture1\", \"perimeter1\", \"area1\", \"smoothness1\", \"compactness1\", \"concavity1\", \"concave_points1\", \"symmetry1\", \"fractal_dimension1\",\n",
    "    \"radius2\", \"texture2\", \"perimeter2\", \"area2\", \"smoothness2\", \"compactness2\", \"concavity2\", \"concave_points2\", \"symmetry2\", \"fractal_dimension2\",\n",
    "    \"radius3\", \"texture3\", \"perimeter3\", \"area3\", \"smoothness3\", \"compactness3\", \"concavity3\", \"concave_points3\", \"symmetry3\", \"fractal_dimension3\"\n",
    "]\n",
    "values = data_line.split(',')[2:]\n",
    "df = pd.DataFrame([values], columns=columns)\n",
    "df = df.astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37ec57",
   "metadata": {},
   "source": [
    "The training data classified this patient's tumor as **Malignant**  \n",
    "So would our model do the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "940a2115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is [100.]% sure that the tumor is Malignant\n"
     ]
    }
   ],
   "source": [
    "model_prediction = sigmoid(df.values @ w + b)\n",
    "if model_prediction > 0.5: \n",
    "    print(f\"the model is {model_prediction * 100}% sure that the tumor is Malignant\") \n",
    "else: print(\"The tumor is benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc55b3b",
   "metadata": {},
   "source": [
    "Perfect!! Now we'll measure how many mistakes does the model make on the whole training set of 500+ patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "58bf2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 569 training examples, the model misclassified 36 of which\n"
     ]
    }
   ],
   "source": [
    "model_error = 0\n",
    "for i in range(x.shape[0]):\n",
    "    f_wb = sigmoid(x[i,:] @ w + b)\n",
    "    model_prediction = 1 if f_wb > 0.5 else 0\n",
    "    if model_prediction != y[i]:\n",
    "        model_error += 1\n",
    "\n",
    "print(f\"Out of {x.shape[0]} training examples, the model misclassified {model_error} of which\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43340872",
   "metadata": {},
   "source": [
    "Meaning that,  \n",
    "In a training set of 569 **real breast cancer patients**, our binary classification model can classify a tumor as malignant or benight correctly **93.67%** of the time!! >:D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f12ba",
   "metadata": {},
   "source": [
    "## Conclusion and ending\n",
    "\n",
    "Thank you for reading this far into my nerdy little program, here we created a logistic regression model where we:\n",
    "- Defined the logistic model with the ``sigmoid(z)`` function\n",
    "- Normalized the data with the ``z_score(x)`` function\n",
    "- Calculated the logarithmic / binary cross-entropy error using ``cost(w, b, x, y)``\n",
    "- Trained and successfuly converged the model with ``compute_gradient(w, b, x, y, alphar, itterations)``\n",
    "\n",
    "==> Written by Ziad Alezzi [Github](https://github.com/lucirie) [linkedin](https://www.linkedin.com/in/ziad-alezzi-8727bb345/)\n",
    "\n",
    "#### This was logistic regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
