{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c89d24",
   "metadata": {},
   "source": [
    "# Multilinear Regression\n",
    "---\n",
    "## Brief:  \n",
    "After taking the **Machine Learning Specialization** by **Andrew NG**, this is an implementation for Multilinear Regression on a dataset that measures student performance in relation to _multiple features_  \n",
    "Where i'll be implementing:\n",
    "\n",
    "- [Data Preparation](#data-preparation)\n",
    "- [Normalization of Data](#normalization)\n",
    "- [Cost Function](#cost-function)\n",
    "- [Gradient Descent](gradient-descent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1e0d2",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- Use pandas to import data into variables\n",
    "- Seperate data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef40854",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('Student_Performance.csv', usecols=[0, 1, 2, 3, 4], sep=',')\n",
    "y = pd.read_csv('Student_Performance.csv', usecols=[5], sep=',')\n",
    "x[\"Extracurricular Activities\"] = x['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "x_train_raw, y_train, x_test_raw, y_test = x[7000:].values, y[7000:].values, x[:3000].values, y[:3000].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144243d7",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "### Goal: Normalizing data using **z-score normalization**\n",
    "\n",
    "This is done since the data in this training set has values all over the place.\n",
    "\n",
    "\n",
    "Before normatization:  \n",
    "- ``x_train_raw = [ 4 90  1  8  4]`` (Some values are large like 90, some values are tiny like 1)\n",
    "\n",
    "After normalization:\n",
    "- ``x_train = [-0.39270927  1.17087605  0.98543933  0.86004793 -0.20328004]`` (All values vary around -1 to 1)\n",
    "\n",
    "\n",
    "The Z-score is calculated using the formula:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $x$ is the original data point\n",
    "- $\\mu$ is the mean\n",
    "- $\\sigma$ is the deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdab98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x):\n",
    "    mu = np.mean(x, axis=0)\n",
    "    sigma = np.std(x, axis=0)\n",
    "    return (x - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4c3b8",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "Here ill be implementing a simple MSE (Mean Squared Error) cost function using the following formula:  \n",
    "\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of data points  \n",
    "- $y_i$ is the true value  \n",
    "- $\\hat{y}_i$ is the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ec6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, b, x, y):\n",
    "    m = len(x)\n",
    "    err = 0.\n",
    "    for i in range(m):\n",
    "        f_wb = x[i,:] @ w + b\n",
    "        err += y[i] - f_wb\n",
    "    err /= m * 2\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a8437",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "- Gradient descent is calulated using this (very fancy and confusing looking mathematical formula):  \n",
    "\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial j(w,b)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial j(w,b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "- But really, in code terms, those confusing derivates are just as simple as this:  \n",
    "$$\n",
    "\\frac{\\partial j(w,b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial j(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "See? its just the cost function. But for the ${x}$ parameter we simply multiply the cost by ${x^i}$ And for ${b}$, its just the cost.  \n",
    "That is how we *update the model's parameters* and successfully *train the model* to fit the data.\n",
    "\n",
    "Notes:\n",
    "- ${j(w,b)}$ is the cost function\n",
    "- ${m}$ is the number of training examples\n",
    "- ${\\hat{y}}$ is the model's prediction f_wb of y\n",
    "- ${y}$ is the target value y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, b, x, y, itterations, alphar):\n",
    "    err_arr = []\n",
    "    w_arr = []\n",
    "    b_arr = []\n",
    "\n",
    "    # Repeat until convergence\n",
    "    for i in range(itterations):\n",
    "        dj_dw = 0.\n",
    "        dj_db = 0.\n",
    "        m = len(x)\n",
    "        for j in range(m):\n",
    "            err = y[j] - (x[j,:] @ w + b)\n",
    "            dj_dw += err * x[i,:]\n",
    "            dj_db += err\n",
    "        dj_dw /= m\n",
    "        dj_db /= m\n",
    "        w = w - alphar * dj_dw\n",
    "        b = b - alphar * dj_db\n",
    "\n",
    "        ## Extra code, for visualization purposes ##\n",
    "        err_arr.append(cost(w, b, x, y))\n",
    "        w_arr.append(w)\n",
    "        b_arr.append(b)\n",
    "        ## Extra code, for visualization purposes ##\n",
    "    return w, b, err_arr, w_arr, b_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f4207",
   "metadata": {},
   "source": [
    "## Testing and Training\n",
    "\n",
    "Now is the final step, combining all made functions.  \n",
    "I will be doing the following:  \n",
    "- Normalizing the training data with the ``z_score()`` function\n",
    "- Testing the model's cost with it's initial parameters using ``compute_cost()`` function\n",
    "- Plotting variation of cost after running gradient descent using ``gradient_descent()`` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = np.random.rand(5, 1), 0.0 # Random parameters\n",
    "x_train, x_test = z_score(x_train_raw), z_score(x_test_raw) # Normalizing training and testing sets\n",
    "\n",
    "w, b, err_arr, w_arr, b_arr = gradient_descent(w, b, x_train, y_train, 200, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(200), err_arr)\n",
    "plt.xlabel(\"Itterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
